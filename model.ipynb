{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aea5136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_community faiss-cpu langchain-groq dotenv langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575254bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcac9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_DATA_FILE = \"normalized_data.json\"\n",
    "COMPANY_FAISS_INDEX_DIR = \"my_faiss_index_company_chunked\"\n",
    "GENERAL_QA_FAISS_INDEX_DIR = \"my_faiss_index_general_qa_chunked\"\n",
    "LLM_MODEL_ID = \"llama-3.1-8b-instant\"\n",
    "GENERAL_QA_DATA_FILE = \"general_qa_data.json\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d0540",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_vectorstore = None\n",
    "general_qa_vectorstore = None\n",
    "embeddings = None\n",
    "llm_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981745dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Loading and Text Splitting ---\n",
    "def load_and_chunk_data(file_path):\n",
    "    \"\"\"Loads JSON data and splits it into LangChain Documents.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "        print(f\"Data: JSON data loaded successfully from '{file_path}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Error: '{file_path}' not found. This should have been handled by dummy data creation or manual upload. Returning empty list.\"\n",
    "        )\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(\n",
    "            f\"Error: Could not decode JSON from '{file_path}'. Please check file format. Returning empty list.\"\n",
    "        )\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data from '{file_path}': {e}. Returning empty list.\")\n",
    "        return []\n",
    "\n",
    "    initial_documents = []\n",
    "    if not raw_data:  # Handle empty JSON file\n",
    "        print(f\"Data: '{file_path}' is empty. No documents to process.\")\n",
    "        return []\n",
    "\n",
    "    for url, text_content in raw_data.items():\n",
    "        # Ensure text_content is a string, as sometimes JSON values can be other types\n",
    "        if isinstance(text_content, str):\n",
    "            initial_documents.append(\n",
    "                Document(page_content=text_content, metadata={\"source\": url})\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Data: Skipping non-string content for URL: {url}\")\n",
    "\n",
    "    if not initial_documents:\n",
    "        print(f\"Data: No valid documents extracted from '{file_path}'.\")\n",
    "        return []\n",
    "\n",
    "    print(\n",
    "        f\"Data: Created {len(initial_documents)} initial LangChain Document objects for '{file_path}'.\"\n",
    "    )\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    print(\n",
    "        f\"Text Splitter: Initialized RecursiveCharacterTextSplitter for '{file_path}'.\"\n",
    "    )\n",
    "\n",
    "    chunked_documents = text_splitter.split_documents(initial_documents)\n",
    "    if not chunked_documents:\n",
    "        print(f\"Text Splitter: No chunks generated from '{file_path}'.\")\n",
    "        return []\n",
    "\n",
    "    print(\n",
    "        f\"Text Splitter: Split {len(initial_documents)} documents into {len(chunked_documents)} chunks for '{file_path}'.\"\n",
    "    )\n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Initialize Embeddings and FAISS ---\n",
    "def initialize_vector_store(chunked_docs, faiss_index_dir):\n",
    "    \"\"\"Initializes embeddings and creates/loads FAISS vector store.\n",
    "\n",
    "    This function takes chunked documents and a specified directory name (faiss_index_dir)\n",
    "    to create or load the FAISS index. It handles creating the directory if it doesn't\n",
    "    exist, ensuring separate storage for different datasets (like company and General Q&A).\n",
    "    \"\"\"\n",
    "    global embeddings\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    print(f\"Initialized HuggingFaceEmbeddings with model: {embeddings.model_name}\")\n",
    "\n",
    "    if not os.path.exists(faiss_index_dir):\n",
    "        os.makedirs(faiss_index_dir)\n",
    "\n",
    "    vectorstore = None\n",
    "    index_file_path = os.path.join(faiss_index_dir, \"index.faiss\")\n",
    "\n",
    "    if (\n",
    "        os.path.exists(faiss_index_dir)\n",
    "        and os.path.isdir(faiss_index_dir)\n",
    "        and os.path.exists(index_file_path)\n",
    "    ):\n",
    "        try:\n",
    "            vectorstore = FAISS.load_local(\n",
    "                faiss_index_dir, embeddings, allow_dangerous_deserialization=True\n",
    "            )\n",
    "            print(f\"FAISS: Index loaded successfully from '{faiss_index_dir}'.\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"FAISS: Error loading existing index from '{faiss_index_dir}': {e}. Attempting to recreate.\"\n",
    "            )\n",
    "            try:\n",
    "                vectorstore = FAISS.from_documents(chunked_docs, embeddings)\n",
    "                vectorstore.save_local(faiss_index_dir)\n",
    "                print(f\"FAISS: Index recreated and saved to '{faiss_index_dir}'.\")\n",
    "            except Exception as create_e:\n",
    "                print(\n",
    "                    f\"FAISS: Failed to recreate index for '{faiss_index_dir}': {create_e}. Returning None.\"\n",
    "                )\n",
    "                return None\n",
    "    else:\n",
    "        print(\n",
    "            f\"FAISS: Directory or index '{faiss_index_dir}' not found. Creating new index...\"\n",
    "        )\n",
    "        try:\n",
    "            vectorstore = FAISS.from_documents(chunked_docs, embeddings)\n",
    "            vectorstore.save_local(faiss_index_dir)\n",
    "            print(f\"FAISS: New index created and saved to '{faiss_index_dir}'.\")\n",
    "        except Exception as create_e:\n",
    "            print(\n",
    "                f\"FAISS: Failed to create new index for '{faiss_index_dir}': {create_e}. Returning None.\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if vectorstore is None:\n",
    "        print(\n",
    "            f\"FAISS: Vector store for '{faiss_index_dir}' is None after initialization attempt. This indicates a problem.\"\n",
    "        )\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm():\n",
    "    \"\"\"Initializes the tokenizer, model, and pipeline for the open-source LLM.\"\"\"\n",
    "    global llm_model\n",
    "\n",
    "    # print(f\" LLM: Initializing Groq ChatModel with model: {LLM_MODEL_ID}...\")\n",
    "    try:\n",
    "        llm_model = ChatGroq(\n",
    "            model=LLM_MODEL_ID,\n",
    "            temperature=0.7,\n",
    "            groq_api_key=GROQ_API_KEY,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # print(f\"LLM: Error initializing Groq ChatModel '{LLM_MODEL_ID}': {e}\")\n",
    "        # print(\"Please ensure your GROQ_API_KEY is set correctly and you have internet access.\")\n",
    "        llm_model = None\n",
    "    return llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Intent Classification Function ---\n",
    "def classify_question_intent(question, llm):\n",
    "    \"\"\"\n",
    "    Classifies the user's question as 'COMPANY', 'GENERAL', or 'OTHER'.\n",
    "    Uses a strict prompt to guide the LLM's classification.\n",
    "    \"\"\"\n",
    "    classification_prompt = f\"\"\"\n",
    "    You are an expert question classifier. Classify the following user question into one of these categories: 'COMPANY', 'GENERAL', 'OTHER'.\n",
    "    - 'COMPANY': Questions specifically about company, their services, products, projects, team, or any business-related aspects of company. This includes follow-up questions about previous company topics.\n",
    "    - 'GENERAL': Common conversational questions like greetings, how are you, current time, general knowledge (e.g., \"what is AI?\"), or anything that is NOT directly about company.\n",
    "    - 'OTHER': Any question that does not clearly fit into 'company' or 'GENERAL' categories, or is entirely out of scope.\n",
    "\n",
    "    Respond with ONLY one word, uppercase: 'COMPANY', 'GENERAL', or 'OTHER'.\n",
    "    Do not add any other text, punctuation, or explanation.\n",
    "\n",
    "    Examples:\n",
    "    Question: \"Hi\"\n",
    "    Category: GENERAL\n",
    "\n",
    "    Question: \"How are you?\"\n",
    "    Category: GENERAL\n",
    "\n",
    "    Question: \"What's up?\"\n",
    "    Category: GENERAL\n",
    "\n",
    "    Question: \"Tell me about company.\"\n",
    "    Category: company\n",
    "\n",
    "    Question: \"What services do they offer?\"\n",
    "    Category: company\n",
    "\n",
    "    Question: \"How are you doing today?\"\n",
    "    Category: GENERAL\n",
    "\n",
    "    Question: \"What is the capital of France?\"\n",
    "    Category: OTHER\n",
    "\n",
    "    Question: \"{question}\"\n",
    "    Category:\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=\"You are a classifier assistant. Respond ONLY with one of the specified categories: 'company', 'GENERAL', 'OTHER'.\"\n",
    "        ),\n",
    "        HumanMessage(content=classification_prompt),\n",
    "    ]\n",
    "    try:\n",
    "        response = llm.invoke(messages)\n",
    "        classification = response.content.strip().upper()\n",
    "        # Basic validation for classification output\n",
    "        if classification in [\"company\", \"GENERAL\", \"OTHER\"]:\n",
    "            print(f\"Intent Classifier: Question classified as '{classification}'.\")\n",
    "            return classification\n",
    "        else:\n",
    "            print(\n",
    "                f\"Intent Classifier: Unexpected classification output: '{classification}'. Defaulting to 'OTHER'.\"\n",
    "            )\n",
    "            return \"OTHER\"\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Intent Classifier: Error during classification: {e}. Defaulting to 'OTHER'.\"\n",
    "        )\n",
    "        return \"OTHER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Chatbot Logic ---\n",
    "def run_chatbot():\n",
    "    \"\"\"Main function to run the command-line chatbot with dual knowledge bases.\"\"\"\n",
    "    global company_vectorstore, general_qa_vectorstore\n",
    "\n",
    "    print(\"\\n--- Initializing Chatbot Components ---\")\n",
    "\n",
    "    # Load and chunk company data, then initialize its vector store\n",
    "    company_chunked_documents = load_and_chunk_data(COMPANY_DATA_FILE)\n",
    "    if not company_chunked_documents:\n",
    "        print(\"Exiting: No company data to process or chunking failed.\")\n",
    "        return\n",
    "    # Use the initialize_vector_store function to create/load the company vector store\n",
    "    company_vectorstore = initialize_vector_store(\n",
    "        company_chunked_documents, COMPANY_FAISS_INDEX_DIR\n",
    "    )\n",
    "    if company_vectorstore is None:\n",
    "        print(\n",
    "            \"Exiting: Failed to initialize company FAISS vector store. Please check errors above.\"\n",
    "        )\n",
    "        return\n",
    "    print(\n",
    "        f\"Global: company_vectorstore is {'initialized' if company_vectorstore else 'NOT initialized'}.\"\n",
    "    )\n",
    "\n",
    "    # Load and chunk General Q&A data, then initialize its vector store\n",
    "    general_qa_chunked_documents = load_and_chunk_data(GENERAL_QA_DATA_FILE)\n",
    "    if not general_qa_chunked_documents:\n",
    "        print(\"Exiting: No General Q&A data to process or chunking failed.\")\n",
    "        return\n",
    "    # Use the initialize_vector_store function to create/load the General Q&A vector store\n",
    "    general_qa_vectorstore = initialize_vector_store(\n",
    "        general_qa_chunked_documents, GENERAL_QA_FAISS_INDEX_DIR\n",
    "    )\n",
    "    if general_qa_vectorstore is None:\n",
    "        print(\n",
    "            \"Exiting: Failed to initialize General Q&A FAISS vector store. Please check errors above.\"\n",
    "        )\n",
    "        return\n",
    "    print(\n",
    "        f\"Global: general_qa_vectorstore is {'initialized' if general_qa_vectorstore else 'NOT initialized'}.\"\n",
    "    )\n",
    "\n",
    "    # Initialize LLM (Groq)\n",
    "    llm_model = initialize_llm()\n",
    "    if llm_model is None:\n",
    "        print(\"Exiting: Failed to initialize LLM. Please check errors above.\")\n",
    "        return\n",
    "    print(f\"Global: llm_model is {'initialized' if llm_model else 'NOT initialized'}.\")\n",
    "\n",
    "    print(\"\\n--- Chatbot Ready! Type 'exit' to quit ---\")\n",
    "    # Initialize chat history for the session\n",
    "    chat_history_messages = []\n",
    "    while True:\n",
    "        user_question = input(\"\\n You: \").strip()\n",
    "        if user_question.lower() == \"exit\":\n",
    "            print(\" Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if not user_question:\n",
    "            print(\" Chatbot: Please type a question.\")\n",
    "            continue\n",
    "\n",
    "        print(\" Chatbot: Thinking...\")\n",
    "        try:\n",
    "            # Step 1: Classify the user's intent\n",
    "            # Ensure LLM is available for classification\n",
    "            if llm_model is None:\n",
    "                print(\"Chatbot: LLM is not initialized. Cannot classify intent.\")\n",
    "                llm_answer = \"I'm sorry, my core components are not ready. Please restart the chatbot.\"\n",
    "                print(f\" Chatbot: {llm_answer}\")\n",
    "                continue\n",
    "\n",
    "            intent = classify_question_intent(user_question, llm_model)\n",
    "\n",
    "            retrieved_docs = []\n",
    "            context = \"\"\n",
    "            current_llm_answer = \"\"\n",
    "            # Step 2: Perform retrieval based on intent\n",
    "            if intent == \"company\":\n",
    "                print(\"Router: Detected 'company' intent.\")\n",
    "                if company_vectorstore:\n",
    "                    retrieved_docs = company_vectorstore.similarity_search(\n",
    "                        user_question, k=6\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Router: company vector store not initialized. Cannot retrieve data.\"\n",
    "                    )\n",
    "            elif intent == \"GENERAL\":\n",
    "                print(\"Router: Detected 'GENERAL' intent.\")\n",
    "                if general_qa_vectorstore:\n",
    "                    retrieved_docs = general_qa_vectorstore.similarity_search(\n",
    "                        user_question, k=5\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Router: General QA vector store not initialized. Cannot retrieve data.\"\n",
    "                    )\n",
    "            else:  # intent == 'OTHER'\n",
    "                print(\"Router: Detected 'OTHER' intent or classification failed.\")\n",
    "                llm_answer = \"I don't know. My knowledge is limited to company and general questions. Please try rephrasing or ask a different question.\"\n",
    "                print(f\" Chatbot: {llm_answer}\")\n",
    "                # Add current user and assistant message to history before continuing\n",
    "                chat_history_messages.append(HumanMessage(content=user_question))\n",
    "                chat_history_messages.append(AIMessage(content=current_llm_answer))\n",
    "                continue\n",
    "\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "            print(\n",
    "                f\"Retrieved {len(retrieved_docs)} chunks from the relevant FAISS index.\"\n",
    "            )\n",
    "\n",
    "            # Step 3: Construct the prompt for the LLM\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                    content=\"You are a helpful and friendly assistant. Answer the user's question based on the provided context. If the information is not present in the context, or if you cannot provide a direct and accurate answer based solely on the context, respond with 'I don't know.' Do not invent information. For general conversational queries, try to give a friendly response even if the information is brief.\"\n",
    "                ),\n",
    "            ]\n",
    "            messages.extend(chat_history_messages)  # Add previous turns\n",
    "            messages.append(\n",
    "                HumanMessage(\n",
    "                    content=f\"Context:\\n{context}\\n\\nQuestion: {user_question}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Step 4: Generate response using the Groq LLM\n",
    "            llm_response = llm_model.invoke(messages)\n",
    "            llm_answer = llm_response.content.strip()\n",
    "\n",
    "            # Step 5: Post-processing to enforce \"I don't know\" strictly\n",
    "\n",
    "            if not context.strip() or len(retrieved_docs) == 0:\n",
    "                llm_answer = \"I don't know. The information to answer your question was not found in my knowledge base.\"\n",
    "            else:\n",
    "                # If LLM gave an answer, and we had context, we assume it's good.\n",
    "                pass\n",
    "\n",
    "            print(f\" Chatbot: {llm_answer}\")\n",
    "            # Update chat history for the next turn with the original user question and the final answer\n",
    "            chat_history_messages.append(HumanMessage(content=user_question))\n",
    "            chat_history_messages.append(AIMessage(content=current_llm_answer))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during chat processing: {e}\")\n",
    "            print(\n",
    "                \" Chatbot: I apologize, but something went wrong. Please try again.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e83dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
